{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "import os\n",
    "import glob\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ignite.engine import Engine, Events, create_supervised_trainer, create_supervised_evaluator\n",
    "from ignite.metrics import Accuracy, Loss, precision, recall, MetricsLambda, confusion_matrix\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from ignite.contrib.handlers import TensorboardLogger, global_step_from_engine, ProgressBar\n",
    "from ignite.contrib.handlers.tensorboard_logger import GradsHistHandler, OutputHandler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['forward', 'backward', 'up', 'down',\n",
    "          'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
    "          'left', 'right', 'go', 'stop', 'yes', 'no', 'on', 'off', 'unknown']\n",
    "# The following dataset labels are considered unkonwn\n",
    "# unknown = ['bed', 'bird', 'cat', 'dog', 'follow', 'happy', 'house', 'learn', 'marvin',\n",
    "#            'sheila', 'visual', 'wow', 'tree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_DATA_ROOT = \"/Users/invincibleo/Leo/Projects/Datasets/SpeechCommands\"\n",
    "# Load the speech command dataset from pytorch dataset\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(os.path.dirname(SPEECH_DATA_ROOT), download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as fileobj:\n",
    "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing split of the data. We do not use validation in this tutorial.\n",
    "train_set = SubsetSC(\"training\")\n",
    "valid_set = SubsetSC(\"validation\")\n",
    "test_set = SubsetSC(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]\n",
    "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
    "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
    "\n",
    "plt.plot(waveform.t().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of labels:\", len(labels))\n",
    "print(\"Number of training examples:\", len(train_set))\n",
    "print(\"Number of validation examples:\", len(valid_set))\n",
    "print(\"Number of testing examples:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    if word in labels:\n",
    "        return torch.tensor(labels.index(word))\n",
    "    else:\n",
    "        return torch.tensor(labels.index(\"unknown\"))\n",
    "\n",
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_count = torch.tensor([ 1251.,  1342.,  2935.,  3121.,  3127.,  3093.,  2956.,  2942.,  3221.,\n",
    "         3074.,  3190.,  3019.,  3158.,  3237.,  3022.,  3006.,  3091.,  3099.,\n",
    "         3221.,  3121.,  3076.,  2951., 20227.])\n",
    "# Customize a weighted random sampler for the training set\n",
    "class WeightedRandomSampler(torch.utils.data.Sampler):\n",
    "    \"\"\"Samples elements from [0,..,len(weights)-1] with given probabilities (weights).\n",
    "\n",
    "    Args:\n",
    "        weights (list) : a list of weights, not necessary summing up to one\n",
    "        num_samples (int) : number of samples to draw\n",
    "        replacement (bool): if True, samples are drawn with replacement.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_count, num_samples, replacement=True):\n",
    "        actual_dist = train_count / torch.sum(train_count)\n",
    "        desired_dist = torch.tensor([1/(len(labels)+4)] * len(labels))\n",
    "        desired_dist[-1] = desired_dist[-1] * 5\n",
    "        print(\"Actual distribution:\", desired_dist)\n",
    "        assert desired_dist[-1] == 5 * desired_dist[0]\n",
    "        self.per_class_weights = desired_dist / actual_dist\n",
    "        self.num_samples = num_samples\n",
    "        self.replacement = replacement\n",
    "\n",
    "        # Assign a weight to each example\n",
    "        # Check if the weights is saved in the disk\n",
    "        if os.path.exists(\"weights_per_sample.pt\"):\n",
    "            self.weights = torch.load(\"weights_per_sample.pt\")\n",
    "        else:\n",
    "            self.weights = torch.tensor([self.per_class_weights[label_to_index(i)] for _, _, i, *_ in train_set])\n",
    "            torch.save(self.weights, \"weights_per_sample.pt\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(torch.multinomial(self.weights, self.num_samples, self.replacement))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual distribution: tensor([0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
      "        0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370, 0.0370,\n",
      "        0.0370, 0.0370, 0.0370, 0.0370, 0.1852])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample_rate = 16000\n",
    "# FILEPATH: /Users/invincibleo/Library/Mobile Documents/com~apple~CloudDocs/Leo/Postdoc/Trips/1_SOUNDS_4TH_Seasonal_school/Project/SOUNDS_4th_school_practical/speech_command_recognition_ex_v3_ignite.ipynb\n",
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "# MFCC feature extraction and save to disk\n",
    "def extract_mfcc(waveform):\n",
    "    mfcc = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=16,\n",
    "        melkwargs={\"n_fft\": int(0.03*sample_rate), \"hop_length\": int(0.03*0.5*sample_rate), \"n_mels\": 64,\n",
    "                   \"window_fn\": torch.hamming_window, \"center\": False, \"pad_mode\": \"reflect\"},\n",
    "    )\n",
    "    return mfcc(waveform)\n",
    "\n",
    "def add_exist_noise(waveform):\n",
    "    # Apply existing noise\n",
    "    noise_files = glob.glob(os.path.join(SPEECH_DATA_ROOT, \"speech_commands_v0.02\", \"_background_noise_\", \"*.wav\"))\n",
    "    noise_file = noise_files[torch.randint(0, len(noise_files), (1,)).item()]\n",
    "    noise_waveform, _ = torchaudio.load(noise_file)\n",
    "    # Make waveform and noise the same length by padding with zeros\n",
    "    if noise_waveform.size(-1) < waveform.size(-1):\n",
    "        noise_waveform = F.pad(noise_waveform, (0, waveform.size(-1) - noise_waveform.size(-1)))\n",
    "    elif noise_waveform.size(-1) >= waveform.size(-1):\n",
    "        # randomly crop noise\n",
    "        max_offset = noise_waveform.size(-1) - waveform.size(-1)\n",
    "        offset = torch.randint(0, max_offset, (1,))\n",
    "        noise_waveform = noise_waveform[..., offset:offset+waveform.size(-1)]\n",
    "    waveform = torchaudio.transforms.AddNoise()(waveform, noise_waveform, snr=torch.randint(-5, 10, (1,)))\n",
    "    return waveform\n",
    "\n",
    "def time_shift(waveform):\n",
    "    # Apply time shift\n",
    "    shift_amount = int(sample_rate*0.3*torch.randint(-1, 1, (1,)).item())\n",
    "    # Apply random time shift to waveform and zero pad at the beginning or at the end\n",
    "    if shift_amount > 0:\n",
    "        waveform = waveform[..., :-shift_amount]\n",
    "        waveform = F.pad(waveform, (shift_amount, 0))\n",
    "    else:\n",
    "        waveform = waveform[..., -shift_amount:]\n",
    "        waveform = F.pad(waveform, (0, -shift_amount))\n",
    "    return waveform\n",
    "\n",
    "# Data Augmentation\n",
    "def data_augment(waveform):\n",
    "    augmentations = [\n",
    "        torchaudio.transforms.Vol(gain=torch.randint(low=-10, high=10, size=(1,)), gain_type='db'),\n",
    "        torchaudio.transforms.TimeMasking(time_mask_param=int(0.02*16000), p=0.1),\n",
    "        lambda w: torchaudio.transforms.AddNoise()(w, torch.randn_like(w), snr=torch.randint(low=-5, high=10, size=(1,))),\n",
    "        add_exist_noise,\n",
    "        time_shift,\n",
    "        lambda w: w\n",
    "    ]\n",
    "    selected_augmentations = random.sample(augmentations, 2)\n",
    "    for augmentation in selected_augmentations:\n",
    "        waveform = augmentation(waveform)\n",
    "    return waveform\n",
    "\n",
    "\n",
    "def collate_fn_extract_feature_train(batch):\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "    tensors, targets = [], []\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, sample_rate, label, speaker_id, utterance_number in batch:\n",
    "        # Apply data augmentation\n",
    "        waveform = data_augment(waveform)\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    # Extract MFCC features\n",
    "    tensors = extract_mfcc(tensors)\n",
    "    # Squeeze and permute \n",
    "    tensors = tensors.squeeze(1).permute(0, 2, 1)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "def collate_fn_extract_feature_eval(batch):\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "    tensors, targets = [], []\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, sample_rate, label, speaker_id, utterance_number in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    # Extract MFCC features\n",
    "    tensors = extract_mfcc(tensors)\n",
    "    # Squeeze and permute \n",
    "    tensors = tensors.squeeze(1).permute(0, 2, 1)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    # shuffle=True,\n",
    "    sampler=WeightedRandomSampler(train_count, num_samples=len(train_set), replacement=True),\n",
    "    collate_fn=collate_fn_extract_feature_train,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "train_eval_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn_extract_feature_eval,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn_extract_feature_eval,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for tensors, targets in train_loader:\n",
    "#         print(index_to_label(targets[0].item()))\n",
    "#         print(tensors.size())\n",
    "#         # # Plot the MFCC feature\n",
    "#         # plt.figure(figsize=(10, 5))\n",
    "#         # plt.imshow(tensors[0].squeeze().numpy(), cmap='hot', interpolation='nearest')\n",
    "#         # plt.title(\"MFCC\")\n",
    "#         # plt.show()\n",
    "#         # Play the audio file\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, n_input=16, n_output=23, n_channel=64):\n",
    "#         super().__init__()\n",
    "#         self.LSTM1 = nn.LSTM(n_input, n_channel, num_layers=1, batch_first=True, bidirectional=True)\n",
    "#         self.fc1 = nn.Linear(n_channel*2, n_output)\n",
    "#         self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x, (h_n, c_n) = self.LSTM1(x)\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.global_avg_pool(torch.transpose(x, 1, 2)).squeeze(2)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# model = LSTM(n_input=16, n_output=len(labels))\n",
    "# model.to(device)\n",
    "# print(model)\n",
    "\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# n = count_parameters(model)\n",
    "# print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_input=16, n_output=23, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv1d(n_input, n_channel, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(3)\n",
    "        self.cnn2 = nn.Conv1d(n_channel, 2*n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(2*n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(3)\n",
    "        self.conv3 = nn.Conv1d(2*n_channel, 4 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(4 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(3)\n",
    "        self.fc1 = nn.Linear(4 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.permute(x, [0, 2, 1])\n",
    "        x = self.pool1(F.relu(self.bn1(self.cnn1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.cnn2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNN(n_input=16, n_output=len(labels))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(80,), stride=(16,))\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=64, out_features=23, bias=True)\n",
      ")\n",
      "Number of parameters: 26135\n"
     ]
    }
   ],
   "source": [
    "# class M5(nn.Module):\n",
    "#     def __init__(self, n_input=1, n_output=35, stride=16, n_channel=32):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=80, stride=stride)\n",
    "#         self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "#         self.pool1 = nn.MaxPool1d(4)\n",
    "#         self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "#         self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "#         self.pool2 = nn.MaxPool1d(4)\n",
    "#         self.conv3 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "#         self.bn3 = nn.BatchNorm1d(2 * n_channel)\n",
    "#         self.pool3 = nn.MaxPool1d(4)\n",
    "#         self.conv4 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "#         self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "#         self.pool4 = nn.MaxPool1d(4)\n",
    "#         self.fc1 = nn.Linear(2 * n_channel, n_output)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(self.bn1(x))\n",
    "#         x = self.pool1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(self.bn2(x))\n",
    "#         x = self.pool2(x)\n",
    "#         x = self.conv3(x)\n",
    "#         x = F.relu(self.bn3(x))\n",
    "#         x = self.pool3(x)\n",
    "#         x = self.conv4(x)\n",
    "#         x = F.relu(self.bn4(x))\n",
    "#         x = self.pool4(x)\n",
    "#         x = F.avg_pool1d(x, x.shape[-1])\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         x = self.fc1(x)\n",
    "#         return F.log_softmax(x, dim=2).squeeze(1)\n",
    "\n",
    "\n",
    "# model = M5(n_input=1, n_output=len(labels))\n",
    "# model.to(device)\n",
    "# print(model)\n",
    "\n",
    "\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# n = count_parameters(model)\n",
    "# print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    y_pred = model(x)\n",
    "    loss = F.nll_loss(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "ProgressBar().attach(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        return y_pred, y\n",
    "    \n",
    "train_evaluator = Engine(validation_step)\n",
    "valid_evaluator = Engine(validation_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach all the evaluation metrics to the evaluators\n",
    "def attach_metrics(evaluator):\n",
    "      Loss(F.nll_loss).attach(evaluator, \"nll\")\n",
    "      Accuracy().attach(evaluator, \"accuracy\")\n",
    "      my_recall = recall.Recall(average=True)\n",
    "      my_recall.attach(evaluator, \"recall\")\n",
    "      my_precision = precision.Precision(average=True)\n",
    "      my_precision.attach(evaluator, \"precision\")\n",
    "      # F1 score is the harmonic mean of precision and recall\n",
    "      # f1 = (MetricsLambda(lambda t: 2*(t[\"precision\"]*t[\"recall\"])/(t[\"precision\"]+t[\"recall\"]), {\"precision\": my_precision, \"recall\": my_recall}))\n",
    "      # f1.attach(evaluator, \"f1\")\n",
    "      \n",
    "      confusion = confusion_matrix.ConfusionMatrix(num_classes=len(labels))\n",
    "      confusion.attach(evaluator, \"cm\")      \n",
    "\n",
    "attach_metrics(train_evaluator)\n",
    "attach_metrics(valid_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_CM_png(cm):\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    fig = plt.figure(figsize=(25, 10))\n",
    "    fig.tight_layout()\n",
    "    # Normalize the confusion matrix\n",
    "    cm = cm / (np.sum(cm.to_numpy(), axis=1)[:, None] + 1e-10)\n",
    "    sn.heatmap(cm, annot=True, cmap='Blues')\n",
    "    # Put x-axis label and y-axis label\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    # Calculate accuracy and put in the title\n",
    "    accuracy = np.trace(cm.to_numpy()) / np.sum(cm.to_numpy())\n",
    "    plt.title(\"Accuracy: {:.1f}%\".format(accuracy*100)) \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tb_log(model, trainer, evaluator, tag, root_dir=\"./tb_logs\"):\n",
    "    # Define a Tensorboard train logger\n",
    "    tb_logger = TensorboardLogger(log_dir=os.path.join(root_dir, tag))\n",
    "\n",
    "    # tb_logger.attach(\n",
    "    #     trainer,\n",
    "    #     event_name=Events.EPOCH_COMPLETED,\n",
    "    #     log_handler=GradsHistHandler(model)\n",
    "    # )\n",
    "    tb_logger.attach_opt_params_handler(\n",
    "        trainer,\n",
    "        event_name=Events.ITERATION_STARTED,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    # Attach handler for plotting both evaluators' metrics after every epoch completes\n",
    "    tb_logger.attach_output_handler(\n",
    "        evaluator,\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        tag=\"\",\n",
    "        metric_names=[\"nll\", \"accuracy\", \"recall\", \"precision\"],\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "    )\n",
    "    # Attach handler to plot the confusion matrix after every epoch completes\n",
    "    @evaluator.on(Events.EPOCH_COMPLETED)\n",
    "    def image_logger():\n",
    "        metrics = evaluator.state.metrics\n",
    "        cm = metrics[\"cm\"]\n",
    "        res = generate_CM_png(cm)\n",
    "        global_step = global_step_from_engine(trainer)(evaluator, Events.EPOCH_COMPLETED)\n",
    "        tb_logger.writer.add_figure(tag=tag, figure=res, global_step=global_step)\n",
    "\n",
    "validate_every = 1\n",
    "# Evaluate on the training set every validate_every epochs\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=validate_every))\n",
    "def run_train_eval():\n",
    "    train_evaluator.run(train_eval_loader)\n",
    "\n",
    "# Evaluate on the validation set every validate_every epochs\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=validate_every))\n",
    "def run_valid_eval():\n",
    "    valid_evaluator.run(valid_loader)\n",
    "\n",
    "# Save the model after every epoch\n",
    "checkpointer = ModelCheckpoint(\n",
    "    \"./models\", \"speech_commands\", n_saved=50, create_dir=True, save_as_state_dict=True, require_empty=False\n",
    ")\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {\"model\": model})\n",
    "\n",
    "tb_log(model, trainer, train_evaluator, tag=\"training\")\n",
    "tb_log(model, trainer, valid_evaluator, tag=\"validation\")\n",
    "    \n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "max_epochs = 60\n",
    "trainer.run(train_loader, max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mode at 44*166 iterations\n",
    "model = M5(n_input=1, n_output=len(labels))\n",
    "model.load_state_dict(torch.load(f\"./models/speech_commands_model_{47*166}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    model.eval()\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    # tensor = transform(tensor)\n",
    "    # if tensor shape is not 16000, then the waveform is too short and we need to pad it\n",
    "    if tensor.numel() != 16000:\n",
    "        tensor = F.pad(tensor, (0, 16000 - tensor.numel()), \"constant\", 0.0)\n",
    "\n",
    "    # # Calculate MFCC\n",
    "    tensor = extract_mfcc(tensor)\n",
    "    tensor = torch.squeeze(tensor, (0, 1)).t()\n",
    "\n",
    "    tensor = model(tensor.unsqueeze(0))\n",
    "    tensor = get_likely_index(tensor)\n",
    "    tensor = index_to_label(tensor.squeeze())\n",
    "    return tensor\n",
    "\n",
    "correct = 0\n",
    "test_pred = []\n",
    "test_target = []\n",
    "for i, (waveform, sample_rate, label, speaker_id, utterance_number) in enumerate(test_set):\n",
    "    output = predict(waveform)\n",
    "    test_pred.append(output)\n",
    "    test_target.append(label)\n",
    "    if output == label:\n",
    "        correct += 1\n",
    "    if output != label:\n",
    "        ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "        print(f\"Data point spk {speaker_id} utt {utterance_number}. Expected: {label}. Predicted: {output}.\")\n",
    "        # break\n",
    "# else:\n",
    "#     print(\"All examples in this dataset were correctly classified!\")\n",
    "#     print(\"In this case, let's just look at the last data point\")\n",
    "#     ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "#     print(f\"Data point #{i}. Expected: {utterance}. Predicted: {output}.\")\n",
    "        \n",
    "print(f\"Accuracy: {correct}/{len(test_set)} ({100. * correct / len(test_set):.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "print(sd.query_devices())\n",
    "sd.default.device = \"Leo's iPhone 13 Microphone\"\n",
    "def record(seconds=5, sample_rate=16000):\n",
    "    # Make a 1s recording\n",
    "    print(\"Start recording.\")\n",
    "    recording = sd.rec(int(seconds * sample_rate), samplerate=sample_rate, channels=1)\n",
    "    sd.wait()\n",
    "    \n",
    "    # Define the file format\n",
    "    fileformat = \"wav\"\n",
    "    filename = f\"_audio.{fileformat}\"\n",
    "    # Write the recording to a file using scipy wavfile\n",
    "    wavfile.write(filename, sample_rate, recording)\n",
    "    return torchaudio.load(filename)\n",
    "\n",
    "# Detect whether notebook runs in google colab\n",
    "record_wav, sample_rate = record()\n",
    "# sample_rate, record_wav = wavfile.read(\"_audio.wav\")\n",
    "# Check if record_wav is a torch tensor\n",
    "if not isinstance(record_wav, torch.Tensor):\n",
    "    record_wav = torch.tensor(record_wav, dtype=torch.float32)\n",
    "record_wav = torch.reshape(record_wav, (1, -1))\n",
    "print(f\"Predicted: {predict(record_wav)}.\")\n",
    "ipd.Audio(record_wav, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(record_wav, torch.Tensor):\n",
    "    record_wav = torch.tensor(record_wav, dtype=torch.float32)\n",
    "record_wav = torch.reshape(record_wav, (1, -1))\n",
    "print(f\"Predicted: {predict(record_wav)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sounds4th",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
