{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech command recognition - Day 3 Part 2\n",
    "## Model improvement\n",
    "******\n",
    "Author: Duowei Tang \\\n",
    "Reference: This exercise is adopted from a Pytorch tutorial: https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html \\\n",
    "Pytorch ignite: https://pytorch-ignite.ai/ \\\n",
    "The Aachen RIR dataset: https://www.iks.rwth-aachen.de/en/research/tools-downloads/databases/aachen-impulse-response-database/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "import glob\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import Accuracy, Loss, Precision, Recall, confusion_matrix\n",
    "from ignite.handlers import ModelCheckpoint, LRScheduler\n",
    "from ignite.contrib.handlers import TensorboardLogger, global_step_from_engine, ProgressBar\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class lables\n",
    "*******\n",
    "In the code block below, it listed the selected command classes and all class labels that are commented out are considered as \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['forward', 'backward', 'up', 'down',\n",
    "          'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
    "          'left', 'right', 'go', 'stop', 'yes', 'no', 'on', 'off', 'unknown']\n",
    "# The following dataset labels are considered unkonwn\n",
    "# unknown = ['bed', 'bird', 'cat', 'dog', 'follow', 'happy', 'house', 'learn', 'marvin',\n",
    "#            'sheila', 'visual', 'wow', 'tree']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the datasets\n",
    "*******\n",
    "Unlike the previous day where we first store the extracted MFCC features to the disk and load the pre-computed features during training. In this exercise, we will extract the features on-the-fly during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_DATA_ROOT = ******The root address where to store the dataset*******\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(os.path.dirname(SPEECH_DATA_ROOT), download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as fileobj:\n",
    "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n",
    "\n",
    "# Create the paritions and features will be extracted during training\n",
    "train_set = SubsetSC(\"training\")\n",
    "valid_set = SubsetSC(\"validation\")\n",
    "test_set = SubsetSC(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    if word in labels:\n",
    "        return torch.tensor(labels.index(word))\n",
    "    else:\n",
    "        return torch.tensor(labels.index(\"unknown\"))\n",
    "\n",
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data sampler\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data per class count is as follows, the code to calculate this is in the appendix\n",
    "train_count = torch.tensor([ 1251.,  1342.,  2935.,  3121.,  3127.,  3093.,  2956.,  2942.,  3221.,\n",
    "         3074.,  3190.,  3019.,  3158.,  3237.,  3022.,  3006.,  3091.,  3099.,\n",
    "         3221.,  3121.,  3076.,  2951., 20227.])\n",
    "# Customize a weighted random sampler for the training set\n",
    "class WeightedRandomSampler(torch.utils.data.Sampler):\n",
    "    \"\"\"Samples elements from [0,..,len(weights)-1] with given probabilities (weights).\n",
    "\n",
    "    Args:\n",
    "        weights (list) : a list of weights, not necessary summing up to one\n",
    "        num_samples (int) : number of samples to draw\n",
    "        replacement (bool): if True, samples are drawn with replacement.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_count, num_samples, replacement=True):\n",
    "        actual_dist = train_count / torch.sum(train_count)\n",
    "\n",
    "        desired_dist = torch.tensor([1/len(labels)] * len(labels))\n",
    "        self.per_class_weights = desired_dist / actual_dist\n",
    "        self.num_samples = num_samples\n",
    "        self.replacement = replacement\n",
    "\n",
    "        # Assign a weight to each example\n",
    "        # Check if the weights is saved in the disk to save time\n",
    "        if os.path.exists(\"weights_per_sample.pt\"):\n",
    "            self.weights = torch.load(\"weights_per_sample.pt\")\n",
    "        else:\n",
    "            self.weights = torch.tensor([self.per_class_weights[label_to_index(i)] for _, _, i, *_ in train_set])\n",
    "            torch.save(self.weights, \"weights_per_sample.pt\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(torch.multinomial(self.weights, self.num_samples, self.replacement))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 16000\n",
    "\n",
    "class MFCC_with_freq_masking(torchaudio.transforms.MFCC):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.freq_masking = torchaudio.transforms.FrequencyMasking(freq_mask_param=5)\n",
    "\n",
    "    def forward(self, waveform):\n",
    "        mel_specgram = self.MelSpectrogram(waveform)\n",
    "        mel_specgram = self.freq_masking(mel_specgram)\n",
    "        if self.log_mels:\n",
    "            log_offset = 1e-6\n",
    "            mel_specgram = torch.log(mel_specgram + log_offset)\n",
    "        else:\n",
    "            mel_specgram = self.amplitude_to_DB(mel_specgram)\n",
    "\n",
    "        # (..., time, n_mels) dot (n_mels, n_mfcc) -> (..., n_nfcc, time)\n",
    "        mfcc = torch.matmul(mel_specgram.transpose(-1, -2), self.dct_mat).transpose(-1, -2)\n",
    "        return mfcc\n",
    "    \n",
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "def extract_mfcc_train(waveform):\n",
    "    mfcc = MFCC_with_freq_masking(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=16,\n",
    "        melkwargs={\"n_fft\": int(0.03*sample_rate), \"hop_length\": int(0.03*0.5*sample_rate), \"n_mels\": 64,\n",
    "                   \"window_fn\": torch.hamming_window, \"center\": False, \"pad_mode\": \"reflect\"},\n",
    "    )\n",
    "    return mfcc(waveform)\n",
    "\n",
    "def extract_mfcc(waveform):\n",
    "    mfcc = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=16,\n",
    "        melkwargs={\"n_fft\": int(0.03*sample_rate), \"hop_length\": int(0.03*0.5*sample_rate), \"n_mels\": 64,\n",
    "                   \"window_fn\": torch.hamming_window, \"center\": False, \"pad_mode\": \"reflect\"},\n",
    "    )\n",
    "    return mfcc(waveform)\n",
    "\n",
    "# Preload the noises to save time\n",
    "noise_files = glob.glob(os.path.join(SPEECH_DATA_ROOT, \"speech_commands_v0.02\", \"_background_noise_\", \"*.wav\"))\n",
    "noise_waveforms = [torchaudio.load(noise_file)[0] for noise_file in noise_files]\n",
    "def add_exist_noise(waveform):\n",
    "    # Apply noise\n",
    "    noise_waveform = noise_waveforms[torch.randint(0, len(noise_waveforms), (1,)).item()]\n",
    "    # Make waveform and noise the same length by padding with zeros\n",
    "    if noise_waveform.size(-1) < waveform.size(-1):\n",
    "        noise_waveform = F.pad(noise_waveform, (0, waveform.size(-1) - noise_waveform.size(-1)))\n",
    "    elif noise_waveform.size(-1) >= waveform.size(-1):\n",
    "        # randomly crop noise\n",
    "        max_offset = noise_waveform.size(-1) - waveform.size(-1)\n",
    "        offset = torch.randint(0, max_offset, (1,))\n",
    "        noise_waveform = noise_waveform[..., offset:offset+waveform.size(-1)]\n",
    "    waveform = torchaudio.transforms.AddNoise()(waveform, noise_waveform, snr=torch.randint(-5, 10, (1,)))\n",
    "    return waveform\n",
    "\n",
    "def perturbate_speed(waveform):\n",
    "    waveform_aug = torchaudio.transforms.SpeedPerturbation(orig_freq=sample_rate, factors=[0.9, 1.1, 1.0, 1.0])(waveform)[0]\n",
    "    if waveform_aug.size(-1) < waveform.size(-1):\n",
    "        waveform_aug = F.pad(waveform_aug, (0, waveform.size(-1) - waveform.size(-1)))\n",
    "    else:\n",
    "        length_diff = waveform_aug.size(-1) - waveform.size(-1)\n",
    "        waveform_aug = waveform_aug[..., length_diff//2:length_diff//2+waveform.size(-1)]\n",
    "    return waveform_aug\n",
    "\n",
    "# Data Augmentation\n",
    "def data_augment(waveform):\n",
    "    augmentations = [\n",
    "        torchaudio.transforms.Vol(gain=torch.randint(low=-10, high=10, size=(1,)), gain_type='db'),\n",
    "        torchaudio.transforms.TimeMasking(time_mask_param=int(0.05*sample_rate), p=1.0),\n",
    "        add_exist_noise,\n",
    "        perturbate_speed,\n",
    "    ]\n",
    "    selected_augmentations = random.sample(augmentations, 1)\n",
    "    for augmentation in selected_augmentations:\n",
    "        waveform = augmentation(waveform)\n",
    "    return waveform\n",
    "\n",
    "def collate_fn_extract_feature_train(batch):\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "    tensors, targets = [], []\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, sample_rate, label, speaker_id, utterance_number in batch:\n",
    "        # Apply data augmentation\n",
    "        waveform = data_augment(waveform)\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    # Extract MFCC features (with frequency masking included)\n",
    "    tensors = extract_mfcc_train(tensors)\n",
    "    # Squeeze and permute \n",
    "    tensors = tensors.squeeze(1).permute(0, 2, 1)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "def collate_fn_extract_feature_eval(batch):\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "    tensors, targets = [], []\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, sample_rate, label, speaker_id, utterance_number in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    # Extract MFCC features\n",
    "    tensors = extract_mfcc(tensors)\n",
    "    # Squeeze and permute \n",
    "    tensors = tensors.squeeze(1).permute(0, 2, 1)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "batch_size = 512\n",
    "num_workers = 8\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    # shuffle=True,\n",
    "    drop_last=True,\n",
    "    sampler=WeightedRandomSampler(train_count, num_samples=len(train_set), replacement=True),\n",
    "    collate_fn=collate_fn_extract_feature_train,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=False,\n",
    "    multiprocessing_context='fork',\n",
    "    persistent_workers=True,\n",
    ")\n",
    "# A separate loader for train evaluation without data augmentation\n",
    "train_eval_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn_extract_feature_eval,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=False,\n",
    "    multiprocessing_context='fork',\n",
    "    persistent_workers=True,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn_extract_feature_eval,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=False,\n",
    "    multiprocessing_context='fork',\n",
    "    persistent_workers=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model design\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bigger model with LSTM\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, n_input=16, n_output=23, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv1d(n_input, n_channel, kernel_size=16)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.cnn2 = nn.Conv1d(n_channel, 2*n_channel, kernel_size=8)\n",
    "        self.bn2 = nn.BatchNorm1d(2*n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        # self.conv3 = nn.Conv1d(2*n_channel, 4 * n_channel, kernel_size=3)\n",
    "        # self.bn3 = nn.BatchNorm1d(4 * n_channel)\n",
    "        # self.pool3 = nn.MaxPool1d(3)\n",
    "        self.lstm = nn.LSTM(2*n_channel, 2*n_channel, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc1 = nn.Linear(4 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.permute(x, [0, 2, 1])\n",
    "        x = F.relu(self.bn1(self.cnn1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.cnn2(x)))\n",
    "        x = self.pool2(x)\n",
    "        # x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # x = self.pool3(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.permute(x, [0, 2, 1])\n",
    "        x, (h_n, c_n) = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = x.squeeze(1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNN_LSTM(n_input=16, n_output=len(labels))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the optimizer and learning rate scheduler\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "torch_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)  # reduce the learning after 20 epochs by a factor of 10\n",
    "scheduler = LRScheduler(torch_lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation using Ignite\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x, y = batch\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    y_pred = model(x)\n",
    "    loss = F.nll_loss(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_step)\n",
    "ProgressBar().attach(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(engine, batch):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        return y_pred, y\n",
    "    \n",
    "train_evaluator = Engine(validation_step)\n",
    "valid_evaluator = Engine(validation_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach evaluation metrics\n",
    "******\n",
    "We will log the loss (i.e. NLL), accuracy, precision, recall confusion matrix metrics to the training and evaluation evaluators. Then we will visualize those quantities in Tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach all the evaluation metrics to the evaluators\n",
    "def attach_metrics(evaluator):\n",
    "      Loss(F.nll_loss).attach(evaluator, \"nll\")\n",
    "      Accuracy().attach(evaluator, \"accuracy\")\n",
    "      my_recall = Recall(average=True)\n",
    "      my_recall.attach(evaluator, \"recall\")\n",
    "      my_precision = Precision(average=True)\n",
    "      my_precision.attach(evaluator, \"precision\")\n",
    "      \n",
    "      my_recall = Recall(average=False)\n",
    "      my_precision = Precision(average=False)\n",
    "      f1 = (my_precision * my_recall * 2 / (my_precision + my_recall)).mean()\n",
    "      f1.attach(evaluator, \"f1\")\n",
    "\n",
    "      confusion = confusion_matrix.ConfusionMatrix(num_classes=len(labels))\n",
    "      confusion.attach(evaluator, \"cm\")      \n",
    "\n",
    "attach_metrics(train_evaluator)\n",
    "attach_metrics(valid_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_CM_png(cm):\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    # Calculate accuracy and put in the title\n",
    "    accuracy = np.trace(cm.to_numpy()) / np.sum(cm.to_numpy())\n",
    "    fig = plt.figure(figsize=(25, 10))\n",
    "    fig.tight_layout()\n",
    "    # Normalize the confusion matrix\n",
    "    cm = cm / (np.sum(cm.to_numpy(), axis=1) + 1e-10)\n",
    "    sn.heatmap(cm, annot=True, cmap='Blues')\n",
    "    # Put x-axis label and y-axis label\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.title(\"Accuracy: {:.1f}%\".format(accuracy*100))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tb_log(model, trainer, evaluator, tag, root_dir=\"./day3_tb_logs_part2_LSTM\"):\n",
    "    # Define a Tensorboard logger\n",
    "    tb_logger = TensorboardLogger(log_dir=os.path.join(root_dir, tag))\n",
    "\n",
    "    # Attach the logger to log learning rate\n",
    "    tb_logger.attach_opt_params_handler(\n",
    "        trainer,\n",
    "        event_name=Events.ITERATION_STARTED,\n",
    "        optimizer=optimizer\n",
    "    )\n",
    "    # Attach handler for plotting both evaluators' metrics after every epoch completes\n",
    "    tb_logger.attach_output_handler(\n",
    "        evaluator,\n",
    "        event_name=Events.EPOCH_COMPLETED,\n",
    "        tag=\"\",\n",
    "        metric_names=[\"nll\", \"accuracy\", \"recall\", \"precision\", \"f1\"],\n",
    "        global_step_transform=global_step_from_engine(trainer),\n",
    "    )\n",
    "    # Attach handler to plot the confusion matrix after every epoch completes\n",
    "    @evaluator.on(Events.EPOCH_COMPLETED)\n",
    "    def image_logger():\n",
    "        metrics = evaluator.state.metrics\n",
    "        cm = metrics[\"cm\"]\n",
    "        res = generate_CM_png(cm)\n",
    "        global_step = global_step_from_engine(trainer)(evaluator, Events.EPOCH_COMPLETED)\n",
    "        tb_logger.writer.add_figure(tag=tag, figure=res, global_step=global_step)\n",
    "\n",
    "# Define the training actions\n",
    "validate_every = 1\n",
    "# It may need more epochs because of the data augmentation\n",
    "max_epochs = 80\n",
    "# Evaluate on the training set every validate_every epochs\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=validate_every))\n",
    "def run_train_eval():\n",
    "    train_evaluator.run(train_eval_loader)\n",
    "\n",
    "# Evaluate on the validation set every validate_every epochs\n",
    "@trainer.on(Events.EPOCH_COMPLETED(every=validate_every))\n",
    "def run_valid_eval():\n",
    "    valid_evaluator.run(valid_loader)\n",
    "\n",
    "# Save the model after every epoch\n",
    "checkpointer = ModelCheckpoint(\n",
    "    \"./day3_tb_logs_part2_LSTM/day3_models\", \"\", n_saved=max_epochs, create_dir=True, save_as_state_dict=True, require_empty=False\n",
    ")\n",
    "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {\"model\": model})\n",
    "\n",
    "# Attach the tensorboard logger\n",
    "tb_log(model, trainer, train_evaluator, tag=\"training\")\n",
    "tb_log(model, trainer, valid_evaluator, tag=\"validation\")\n",
    "    \n",
    "# Attach the learning rate scheduler to the trainer to adjust the learning rate\n",
    "trainer.add_event_handler(Events.EPOCH_STARTED, scheduler)\n",
    "\n",
    "# Run the trainer\n",
    "trainer.run(train_loader, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on the testing set\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please select the model based on the validation metrics and fill in the model path here\n",
    "test_epoch=70\n",
    "model = CNN_LSTM(n_input=16, n_output=len(labels))\n",
    "model.load_state_dict(torch.load(f\"day3_tb_logs_part2_LSTM/day3_models/model_{test_epoch*165}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    model.eval()\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    # tensor = transform(tensor)\n",
    "    # if tensor shape is not 16000, then the waveform is too short and we need to pad it\n",
    "    if tensor.numel() != 16000:\n",
    "        tensor = F.pad(tensor, (0, 16000 - tensor.numel()), \"constant\", 0.0)\n",
    "\n",
    "    # # Calculate MFCC\n",
    "    tensor = extract_mfcc(tensor)\n",
    "    tensor = torch.squeeze(tensor, (0, 1)).t()\n",
    "\n",
    "    tensor = model(tensor.unsqueeze(0))\n",
    "    tensor = get_likely_index(tensor)\n",
    "    tensor = index_to_label(tensor.squeeze())\n",
    "    return tensor\n",
    "\n",
    "correct = 0\n",
    "test_pred = []\n",
    "test_target = []\n",
    "for i, (waveform, sample_rate, label, speaker_id, utterance_number) in enumerate(test_set):\n",
    "    output = predict(waveform)\n",
    "    test_pred.append(output)\n",
    "    test_target.append(label)\n",
    "    if label in labels:\n",
    "        if output == label:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(f\"Data point spk {speaker_id} utt {utterance_number}. Expected: {label}. Predicted: {output}.\")\n",
    "    else:\n",
    "        if output == \"unknown\":\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(f\"Data point spk {speaker_id} utt {utterance_number}. Expected: unknown. Predicted: {output}.\")\n",
    "        \n",
    "print(f\"Accuracy: {correct}/{len(test_set)} ({100. * correct / len(test_set):.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with your own voice\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "print(sd.query_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please chose the device id from the list of devices above, and fill in the device id here\n",
    "# E.g. sd.default.device = \"MacBook Pro Microphone\"\n",
    "sd.default.device = ***CHOSE THE DEVICE ID HERE***\n",
    "def record(seconds=5, sample_rate=16000):\n",
    "    # Make a 1s recording\n",
    "    print(\"Start recording.\")\n",
    "    recording = sd.rec(int(seconds * sample_rate), samplerate=sample_rate, channels=1)\n",
    "    sd.wait()\n",
    "    \n",
    "    # Define the file format\n",
    "    fileformat = \"wav\"\n",
    "    filename = f\"_audio.{fileformat}\"\n",
    "    # Write the recording to a file using scipy wavfile\n",
    "    wavfile.write(filename, sample_rate, recording)\n",
    "    return torchaudio.load(filename)\n",
    "\n",
    "# Detect whether notebook runs in google colab\n",
    "record_wav, sample_rate = record()\n",
    "# sample_rate, record_wav = wavfile.read(\"_audio.wav\")\n",
    "# Check if record_wav is a torch tensor\n",
    "if not isinstance(record_wav, torch.Tensor):\n",
    "    record_wav = torch.tensor(record_wav, dtype=torch.float32)\n",
    "record_wav = torch.reshape(record_wav, (1, -1))\n",
    "print(f\"Predicted: {predict(record_wav)}.\")\n",
    "ipd.Audio(record_wav, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Appendix\n",
    "*******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of training examples per label\n",
    "# labels_train = []\n",
    "# for _, label in train_loader:\n",
    "#     labels_train.append(label)\n",
    "\n",
    "# labels_train = torch.stack(labels_train)\n",
    "# labels_train = labels_train.view(-1)\n",
    "\n",
    "# print(\"Shape of labels_train:\", labels_train.size())\n",
    "\n",
    "# # Count the number of training examples per label\n",
    "# train_count = torch.bincount(labels_train).float()\n",
    "# print(\"Count of labels:\", train_count)\n",
    "\n",
    "# # Plot the count of train examples per label\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.bar(torch.arange(len(train_count)), train_count.numpy())\n",
    "# plt.xticks(torch.arange(len(train_count)), labels, rotation=45)\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.xlabel(\"Label\")\n",
    "# plt.title(\"Number of training examples per label\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bigger model with LSTM\n",
    "# class CNN(nn.Module):\n",
    "#     def __init__(self, n_input=16, n_output=23, n_channel=32):\n",
    "#         super().__init__()\n",
    "#         self.cnn1 = nn.Conv1d(n_input, n_channel, kernel_size=16)\n",
    "#         self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "#         self.pool1 = nn.MaxPool1d(2)\n",
    "#         self.cnn2 = nn.Conv1d(n_channel, 2*n_channel, kernel_size=8)\n",
    "#         self.bn2 = nn.BatchNorm1d(2*n_channel)\n",
    "#         self.pool2 = nn.MaxPool1d(2)\n",
    "#         # self.conv3 = nn.Conv1d(2*n_channel, 4 * n_channel, kernel_size=3)\n",
    "#         # self.bn3 = nn.BatchNorm1d(4 * n_channel)\n",
    "#         # self.pool3 = nn.MaxPool1d(3)\n",
    "#         self.lstm = nn.LSTM(2*n_channel, 2*n_channel, num_layers=1, batch_first=True, bidirectional=True)\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "#         self.fc1 = nn.Linear(4 * n_channel, n_output)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.permute(x, [0, 2, 1])\n",
    "#         x = F.relu(self.bn1(self.cnn1(x)))\n",
    "#         x = self.pool1(x)\n",
    "#         x = F.relu(self.bn2(self.cnn2(x)))\n",
    "#         x = self.pool2(x)\n",
    "#         # x = F.relu(self.bn3(self.conv3(x)))\n",
    "#         # x = self.pool3(x)\n",
    "#         x = self.dropout(x)\n",
    "#         x = torch.permute(x, [0, 2, 1])\n",
    "#         x, (h_n, c_n) = self.lstm(x)\n",
    "#         x = x[:, -1, :]\n",
    "#         x = x.squeeze(1)\n",
    "#         x = self.fc1(x)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "\n",
    "# model = CNN(n_input=16, n_output=len(labels))\n",
    "# model.to(device)\n",
    "# print(model)\n",
    "\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# n = count_parameters(model)\n",
    "# print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach a logger to log model's gradients distribution\n",
    "# tb_logger.attach(\n",
    "#     trainer,\n",
    "#     event_name=Events.EPOCH_COMPLETED,\n",
    "#     log_handler=GradsHistHandler(model)\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sounds4th",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
