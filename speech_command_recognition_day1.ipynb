{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech command recognition - Day 1\n",
    "## Build your first model\n",
    "******\n",
    "Author: Duowei Tang \\\n",
    "Reference: This exercise is adopted from a Pytorch tutorial: https://pytorch.org/tutorials/intermediate/speech_command_classification_with_torchaudio_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class lables\n",
    "*******\n",
    "In the code block below, it listed the selected command classes and all class labels that are commented out are considered as \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['forward', 'backward', 'up', 'down',\n",
    "          'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
    "          'left', 'right', 'go', 'stop', 'yes', 'no', 'on', 'off', 'unknown']\n",
    "# The following dataset labels are considered unkonwn\n",
    "# unknown = ['bed', 'bird', 'cat', 'dog', 'follow', 'happy', 'house', 'learn', 'marvin',\n",
    "#            'sheila', 'visual', 'wow', 'tree']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_DATA_ROOT = ******The root address where to store the dataset*******\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(os.path.dirname(SPEECH_DATA_ROOT), download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as fileobj:\n",
    "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the partitions\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First time running this will download a dataset of 2.3 GB to the current directory\n",
    "# And takes about 5 minutes to download, and 5 minutes to unzip\n",
    "train_set = SubsetSC(\"training\")\n",
    "valid_set = SubsetSC(\"validation\")\n",
    "test_set = SubsetSC(\"testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get some ideas about the dataset\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A data tuple has the form:\n",
    "# waveform, sample_rate, label, speaker_id, utterance_number\n",
    "waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]\n",
    "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
    "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
    "\n",
    "plt.plot(waveform.t().numpy())\n",
    "print(\"Label: {}\".format(label))\n",
    "ipd.Audio(waveform.numpy(), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of labels:\", len(labels))\n",
    "print(\"Number of training examples:\", len(train_set))\n",
    "print(\"Number of validation examples:\", len(valid_set))\n",
    "print(\"Number of testing examples:\", len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label transformation\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_index(word):\n",
    "    if word in labels:\n",
    "        return torch.tensor(labels.index(word))\n",
    "    else:\n",
    "        return torch.tensor(labels.index(\"unknown\"))\n",
    "\n",
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return labels[index]\n",
    "\n",
    "# Test label \"on\"\n",
    "index = label_to_index(\"on\")\n",
    "word = index_to_label(index)\n",
    "print(index, word)\n",
    "\n",
    "# Test label \"unknown\"\n",
    "index = label_to_index(\"bird\")\n",
    "word = index_to_label(index)\n",
    "print(index, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC feature extraction\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "# MFCC feature extraction and save to disk\n",
    "def extract_mfcc(waveform):\n",
    "    mfcc = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=16,\n",
    "        melkwargs={\"n_fft\": int(0.03*sample_rate), \"hop_length\": int(0.03*0.5*sample_rate), \"n_mels\": 64,\n",
    "                   \"window_fn\": torch.hamming_window, \"center\": False, \"pad_mode\": \"reflect\"},\n",
    "    )\n",
    "    return mfcc(waveform)\n",
    "\n",
    "def collate_fn_extract_feature(batch):\n",
    "\n",
    "    tensors, targets, file_name = [], [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, sample_rate, label, speaker_id, utterance_number in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label]\n",
    "        name = \"mfcc_\" + str(speaker_id) + \"_\" + str(utterance_number) + \".pt\"\n",
    "        file_name += [name]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    # Extract MFCC features\n",
    "    tensors = extract_mfcc(tensors)\n",
    "\n",
    "    return tensors, targets, file_name\n",
    "\n",
    "batch_size = 1\n",
    "num_workers = 8\n",
    "pin_memory = False\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_extract_feature,\n",
    "    pin_memory=pin_memory,\n",
    "    multiprocessing_context=\"fork\",\n",
    "    persistent_workers=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn_extract_feature,\n",
    "    pin_memory=pin_memory,\n",
    "    multiprocessing_context=\"fork\",\n",
    "    persistent_workers=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below only needs to be run once! It will save the MFCC features to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only extract for training and validation set\n",
    "mfcc_feature_dir = os.path.join(SPEECH_DATA_ROOT, \"mfcc\")\n",
    "os.makedirs(mfcc_feature_dir, exist_ok=True)\n",
    "for feature, label, file_name in tqdm(train_loader):\n",
    "    label = label[0]\n",
    "    os.makedirs(os.path.join(mfcc_feature_dir, label), exist_ok=True)\n",
    "    feature_path = os.path.join(mfcc_feature_dir, label, file_name[0])\n",
    "    if not os.path.exists(feature_path):\n",
    "        torch.save(feature, feature_path)\n",
    "\n",
    "for feature, label, file_name in tqdm(valid_loader):\n",
    "    label = label[0]\n",
    "    os.makedirs(os.path.join(mfcc_feature_dir, label), exist_ok=True)\n",
    "    feature_path = os.path.join(mfcc_feature_dir, label, file_name[0])\n",
    "    if not os.path.exists(feature_path):\n",
    "        torch.save(feature, feature_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MFCC features as dataset\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(os.path.dirname(SPEECH_DATA_ROOT), download=False)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            feature_path_list = []\n",
    "            with open(filepath) as fileobj:\n",
    "                for line in fileobj:\n",
    "                    line = line.strip().replace(\"_nohash_\", \"_\")\n",
    "                    mfcc_file_name = \"mfcc_\" + line.split(\"/\")[-1].split(\".\")[0] + \".pt\"\n",
    "                    mfcc_file_name = os.path.join(line.split(\"/\")[0], mfcc_file_name)\n",
    "                    feature_path = os.path.join(SPEECH_DATA_ROOT, \"mfcc\", mfcc_file_name)\n",
    "                    feature_path_list.append(feature_path)\n",
    "            return feature_path_list\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = set(load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\"))\n",
    "            walker = sorted(str(p) for p in glob.glob(os.path.join(SPEECH_DATA_ROOT, \"mfcc\", \"*\", \"*.pt\")))\n",
    "            self._walker = [\n",
    "                w for w in walker\n",
    "                if os.path.normpath(w) not in excludes\n",
    "            ]\n",
    "\n",
    "    def __getitem__(self, n: int):\n",
    "        fileid = self._walker[n]\n",
    "        feature = torch.load(fileid)\n",
    "        label = fileid.split(\"/\")[-2]\n",
    "        return feature, label\n",
    "    \n",
    "\n",
    "train_set = MFCC(\"training\")\n",
    "valid_set = MFCC(\"validation\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    tensors, targets = [], []\n",
    "    for mfcc, label in batch:\n",
    "        mfcc = torch.squeeze(mfcc, (0, 1))\n",
    "        tensors += [mfcc]\n",
    "        targets += [label_to_index(label)]\n",
    "\n",
    "    tensors = pad_sequence(tensors)\n",
    "    tensors = tensors.permute(0, 2, 1)\n",
    "    targets = torch.stack(targets)\n",
    "    return tensors, targets\n",
    "\n",
    "# Construct the dataloaders\n",
    "batch_size = 512\n",
    "num_workers = 4\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    multiprocessing_context=\"fork\",\n",
    "    persistent_workers=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=False,\n",
    "    multiprocessing_context=\"fork\",\n",
    "    persistent_workers=True,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model design\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_input=16, n_output=23, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv1d(n_input, n_channel, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(3)\n",
    "        self.cnn2 = nn.Conv1d(n_channel, 2*n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(2*n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(3)\n",
    "        self.conv3 = nn.Conv1d(2*n_channel, 4 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(4 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(3)\n",
    "        self.fc1 = nn.Linear(4 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.permute(x, [0, 2, 1])\n",
    "        x = self.pool1(F.relu(self.bn1(self.cnn1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.cnn2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNN(n_input=16, n_output=len(labels))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the optimizer and learning rate scheduler\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "        # negative log-likelihood for a tensor\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # record loss\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    # Calculate training set accuracy\n",
    "    acc = 100. * correct / len(train_loader.dataset)\n",
    "    print(f\"\\nTraining set: Average loss: {sum(losses) / len(losses):.4f}, Accuracy: {correct}/{len(train_loader.dataset)} ({acc:.0f}%)\\n\")\n",
    "    return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "    acc = 100. * correct / len(valid_loader.dataset)\n",
    "    print(f\"\\nValidation Epoch: {epoch}\\tAccuracy: {correct}/{len(valid_loader.dataset)} ({acc:.0f}%)\\n\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 20\n",
    "n_epoch = 40\n",
    "\n",
    "train_accuracy_epoch = []\n",
    "valid_accuracy_epoch = []\n",
    "\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        train_accuracy_epoch.append(train(model, epoch, log_interval))\n",
    "        valid_accuracy_epoch.append(validation(model, epoch))\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save model\n",
    "        os.makedirs(\"./day1_models\", exist_ok=True)\n",
    "        torch.save(model.state_dict(), f\"./day1_models/model_e{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracies vs epoch in the same plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_accuracy_epoch, label=\"Train\")\n",
    "plt.plot(valid_accuracy_epoch, label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model on the testing set\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mode at 44*166 iterations\n",
    "model = CNN(n_input=16, n_output=len(labels))\n",
    "model.load_state_dict(torch.load(f\"./v2_models/model_e37.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    model.eval()\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    # if tensor shape is not 16000, then the waveform is too short and we need to pad it\n",
    "    if tensor.numel() != 16000:\n",
    "        tensor = F.pad(tensor, (0, 16000 - tensor.numel()), \"constant\", 0.0)\n",
    "\n",
    "    # # Calculate MFCC\n",
    "    tensor = extract_mfcc(tensor)\n",
    "    tensor = torch.squeeze(tensor, (0, 1)).t()\n",
    "\n",
    "    tensor = model(tensor.unsqueeze(0))\n",
    "    tensor = get_likely_index(tensor)\n",
    "    tensor = index_to_label(tensor.squeeze())\n",
    "    return tensor\n",
    "\n",
    "correct = 0\n",
    "test_pred = []\n",
    "test_target = []\n",
    "for i, (waveform, sample_rate, label, speaker_id, utterance_number) in enumerate(test_set):\n",
    "    output = predict(waveform)\n",
    "    test_pred.append(output)\n",
    "    test_target.append(label)\n",
    "    if label in labels:\n",
    "        if output == label:\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(f\"Data point spk {speaker_id} utt {utterance_number}. Expected: {label}. Predicted: {output}.\")\n",
    "    else:\n",
    "        if output == \"unknown\":\n",
    "            correct += 1\n",
    "        else:\n",
    "            print(f\"Data point spk {speaker_id} utt {utterance_number}. Expected: {label}. Predicted: {output}.\")\n",
    "        # break\n",
    "        \n",
    "print(f\"Accuracy: {correct}/{len(test_set)} ({100. * correct / len(test_set):.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model on your own voice\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "print(sd.query_devices())\n",
    "sd.default.device = ***CHOSE THE DEVICE ID HERE***\n",
    "def record(seconds=5, sample_rate=16000):\n",
    "    # Make a 1s recording\n",
    "    print(\"Start recording.\")\n",
    "    recording = sd.rec(int(seconds * sample_rate), samplerate=sample_rate, channels=1)\n",
    "    sd.wait()\n",
    "    \n",
    "    # Define the file format\n",
    "    fileformat = \"wav\"\n",
    "    filename = f\"_audio.{fileformat}\"\n",
    "    # Write the recording to a file using scipy wavfile\n",
    "    wavfile.write(filename, sample_rate, recording)\n",
    "    return torchaudio.load(filename)\n",
    "\n",
    "# Detect whether notebook runs in google colab\n",
    "record_wav, sample_rate = record()\n",
    "# sample_rate, record_wav = wavfile.read(\"_audio.wav\")\n",
    "# Check if record_wav is a torch tensor\n",
    "if not isinstance(record_wav, torch.Tensor):\n",
    "    record_wav = torch.tensor(record_wav, dtype=torch.float32)\n",
    "record_wav = torch.reshape(record_wav, (1, -1))\n",
    "print(f\"Predicted: {predict(record_wav)}.\")\n",
    "ipd.Audio(record_wav, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Appendix\n",
    "******\n",
    "1. Code to compute number of example in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of training examples per label\n",
    "# labels_train = []\n",
    "# for _, label in train_loader:\n",
    "#     labels_train.append(label)\n",
    "\n",
    "# labels_train = torch.stack(labels_train)\n",
    "# labels_train = labels_train.view(-1)\n",
    "\n",
    "# print(\"Shape of labels_train:\", labels_train.size())\n",
    "\n",
    "# # Count the number of training examples per label\n",
    "# train_count = torch.bincount(labels_train).float()\n",
    "# print(\"Count of labels:\", train_count)\n",
    "\n",
    "# # Plot the count of train examples per label\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.bar(torch.arange(len(train_count)), train_count.numpy())\n",
    "# plt.xticks(torch.arange(len(train_count)), labels, rotation=45)\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.xlabel(\"Label\")\n",
    "# plt.title(\"Number of training examples per label\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sounds4th",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
