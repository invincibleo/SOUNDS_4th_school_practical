{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from scipy.io import wavfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "import os\n",
    "import glob\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['forward', 'backward', 'up', 'down',\n",
    "          'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'zero',\n",
    "          'left', 'right', 'go', 'stop', 'yes', 'no', 'on', 'off', 'unknown']\n",
    "# The following dataset labels are considered unkonwn\n",
    "# unknown = ['bed', 'bird', 'cat', 'dog', 'follow', 'happy', 'house', 'learn', 'marvin',\n",
    "#            'sheila', 'visual', 'wow', 'tree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_DATA_ROOT = \"/Users/invincibleo/Leo/Projects/Datasets/SpeechCommands\"\n",
    "# Load the speech command dataset from pytorch dataset\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(os.path.dirname(SPEECH_DATA_ROOT), download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as fileobj:\n",
    "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in fileobj]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing split of the data. We do not use validation in this tutorial.\n",
    "train_set = SubsetSC(\"training\")\n",
    "valid_set = SubsetSC(\"validation\")\n",
    "test_set = SubsetSC(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate, label, speaker_id, utterance_number = train_set[0]\n",
    "print(\"Shape of waveform: {}\".format(waveform.size()))\n",
    "print(\"Sample rate of waveform: {}\".format(sample_rate))\n",
    "\n",
    "plt.plot(waveform.t().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of labels:\", len(labels))\n",
    "print(\"Number of training examples:\", len(train_set))\n",
    "print(\"Number of validation examples:\", len(valid_set))\n",
    "print(\"Number of testing examples:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20) on\n",
      "tensor(22) unknown\n"
     ]
    }
   ],
   "source": [
    "def label_to_index(word):\n",
    "    if word in labels:\n",
    "        return torch.tensor(labels.index(word))\n",
    "    else:\n",
    "        return torch.tensor(labels.index(\"unknown\"))\n",
    "\n",
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return labels[index]\n",
    "\n",
    "# Test label \"on\"\n",
    "index = label_to_index(\"on\")\n",
    "word = index_to_label(index)\n",
    "print(index, word)\n",
    "\n",
    "# Test label \"unknown\"\n",
    "index = label_to_index(\"bird\")\n",
    "word = index_to_label(index)\n",
    "print(index, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = [item.t() for item in batch]\n",
    "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch.permute(0, 2, 1)\n",
    "\n",
    "# MFCC feature extraction and save to disk\n",
    "def extract_mfcc(waveform):\n",
    "    mfcc = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=16,\n",
    "        melkwargs={\"n_fft\": int(0.03*sample_rate), \"hop_length\": int(0.03*0.5*sample_rate), \"n_mels\": 64,\n",
    "                   \"window_fn\": torch.hamming_window, \"center\": False, \"pad_mode\": \"reflect\"},\n",
    "    )\n",
    "    return mfcc(waveform)\n",
    "\n",
    "def collate_fn_extract_feature(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, sample_rate, label, speaker_id, utterance_number\n",
    "\n",
    "    tensors, targets, file_name = [], [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, sample_rate, label, speaker_id, utterance_number in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label_to_index(label)]\n",
    "        name = \"mfcc_\" + str(speaker_id) + \"_\" + str(utterance_number) + \".pt\"\n",
    "        file_name += [name]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence(tensors)\n",
    "    # Extract MFCC features\n",
    "    tensors = extract_mfcc(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets, file_name\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_extract_feature,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn_extract_feature,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_feature_dir = os.path.join(SPEECH_DATA_ROOT, \"mfcc\")\n",
    "os.makedirs(mfcc_feature_dir, exist_ok=True)\n",
    "for feature, label, file_name in tqdm(train_loader):\n",
    "    label = index_to_label(label[0])\n",
    "    os.makedirs(os.path.join(mfcc_feature_dir, label), exist_ok=True)\n",
    "    feature_path = os.path.join(mfcc_feature_dir, label, file_name[0])\n",
    "    if not os.path.exists(feature_path):\n",
    "        torch.save(feature, feature_path)\n",
    "\n",
    "for feature, label, file_name in tqdm(valid_loader):\n",
    "    label = index_to_label(label[0])\n",
    "    os.makedirs(os.path.join(mfcc_feature_dir, label), exist_ok=True)\n",
    "    feature_path = os.path.join(mfcc_feature_dir, label, file_name[0])\n",
    "    if not os.path.exists(feature_path):\n",
    "        torch.save(feature, feature_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_DATA_ROOT = \"/Users/invincibleo/Leo/Projects/Datasets/SpeechCommands\"\n",
    "class MFCC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(os.path.dirname(SPEECH_DATA_ROOT), download=False)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            feature_path_list = []\n",
    "            with open(filepath) as fileobj:\n",
    "                for line in fileobj:\n",
    "                    line = line.strip().replace(\"_nohash_\", \"_\")\n",
    "                    mfcc_file_name = \"mfcc_\" + line.split(\"/\")[-1].split(\".\")[0] + \".pt\"\n",
    "                    mfcc_file_name = os.path.join(line.split(\"/\")[0], mfcc_file_name)\n",
    "                    feature_path = os.path.join(SPEECH_DATA_ROOT, \"mfcc\", mfcc_file_name)\n",
    "                    feature_path_list.append(feature_path)\n",
    "            return feature_path_list\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = set(load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\"))\n",
    "            walker = sorted(str(p) for p in glob.glob(os.path.join(SPEECH_DATA_ROOT, \"mfcc\", \"*\", \"*.pt\")))\n",
    "            self._walker = [\n",
    "                w for w in walker\n",
    "                if os.path.normpath(w) not in excludes\n",
    "            ]\n",
    "\n",
    "    def __getitem__(self, n: int):\n",
    "        fileid = self._walker[n]\n",
    "        feature = torch.load(fileid)\n",
    "        label = fileid.split(\"/\")[-2]\n",
    "        return feature, label\n",
    "    \n",
    "\n",
    "train_set = MFCC(\"training\")\n",
    "valid_set = MFCC(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    tensors, targets = [], []\n",
    "    for mfcc, label in batch:\n",
    "        mfcc = torch.squeeze(mfcc, (0, 1))\n",
    "        tensors += [mfcc]\n",
    "        targets += [label_to_index(label)]\n",
    "\n",
    "    tensors = pad_sequence(tensors)\n",
    "    tensors = tensors.permute(0, 2, 1)\n",
    "    targets = torch.stack(targets)\n",
    "    return tensors, targets\n",
    "\n",
    "# Construct the dataloaders\n",
    "batch_size = 512\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    drop_last=False,\n",
    ")\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     test_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=collate_fn,\n",
    "#     drop_last=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Count the number of training examples per label\n",
    "# labels_train = []\n",
    "# for _, label in train_loader:\n",
    "#     labels_train.append(label)\n",
    "\n",
    "# labels_train = torch.stack(labels_train)\n",
    "# labels_train = labels_train.view(-1)\n",
    "\n",
    "# print(\"Shape of labels_train:\", labels_train.size())\n",
    "\n",
    "# # Count the number of training examples per label\n",
    "# train_count = torch.bincount(labels_train).float()\n",
    "# print(\"Count of labels:\", train_count)\n",
    "\n",
    "# # Plot the count of train examples per label\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.bar(torch.arange(len(train_count)), train_count.numpy())\n",
    "# plt.xticks(torch.arange(len(train_count)), labels, rotation=45)\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.xlabel(\"Label\")\n",
    "# plt.title(\"Number of training examples per label\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_input=16, n_output=23, n_channel=64):\n",
    "        super().__init__()\n",
    "        self.LSTM1 = nn.LSTM(n_input, n_channel, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(n_channel*2, n_output)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (h_n, c_n) = self.LSTM1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.global_avg_pool(torch.transpose(x, 1, 2)).squeeze(2)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "mfcc, label = train_set[0]\n",
    "model = LSTM(n_input=mfcc.shape[-2], n_output=len(labels))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (cnn1): Conv1d(16, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (cnn2): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(64, 128, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=128, out_features=23, bias=True)\n",
      ")\n",
      "Number of parameters: 35895\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, n_input=16, n_output=23, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.cnn1 = nn.Conv1d(n_input, n_channel, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(3)\n",
    "        self.cnn2 = nn.Conv1d(n_channel, 2*n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(2*n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(3)\n",
    "        self.conv3 = nn.Conv1d(2*n_channel, 4 * n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(4 * n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(3)\n",
    "        self.fc1 = nn.Linear(4 * n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.permute(x, [0, 2, 1])\n",
    "        x = self.pool1(F.relu(self.bn1(self.cnn1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.cnn2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = CNN(n_input=16, n_output=len(labels))\n",
    "model.to(device)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        # data = transform(data)\n",
    "        output = model(data)\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        loss = F.nll_loss(output.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        # pbar.update(pbar_update)\n",
    "        # record loss\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    # Calculate training set accuracy\n",
    "    acc = 100. * correct / len(train_loader.dataset)\n",
    "    print(f\"\\nTraining set: Average loss: {sum(losses) / len(losses):.4f}, Accuracy: {correct}/{len(train_loader.dataset)} ({acc:.0f}%)\\n\")\n",
    "    return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, epoch):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data, target in valid_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        # data = transform(data)\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target)\n",
    "\n",
    "        # update progress bar\n",
    "        # pbar.update(pbar_update)\n",
    "\n",
    "    acc = 100. * correct / len(valid_loader.dataset)\n",
    "    print(f\"\\nValidation Epoch: {epoch}\\tAccuracy: {correct}/{len(valid_loader.dataset)} ({acc:.0f}%)\\n\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/84843 (0%)]\tLoss: 3.280340\n",
      "Train Epoch: 1 [10240/84843 (12%)]\tLoss: 1.255801\n",
      "Train Epoch: 1 [20480/84843 (24%)]\tLoss: 0.984637\n",
      "Train Epoch: 1 [30720/84843 (36%)]\tLoss: 0.688828\n",
      "Train Epoch: 1 [40960/84843 (48%)]\tLoss: 0.823618\n",
      "Train Epoch: 1 [51200/84843 (61%)]\tLoss: 0.636571\n",
      "Train Epoch: 1 [61440/84843 (73%)]\tLoss: 0.560820\n",
      "Train Epoch: 1 [71680/84843 (85%)]\tLoss: 0.681125\n",
      "Train Epoch: 1 [81920/84843 (97%)]\tLoss: 0.577117\n",
      "\n",
      "Training set: Average loss: 0.8764, Accuracy: 61860/84843 (73%)\n",
      "\n",
      "\n",
      "Validation Epoch: 1\tAccuracy: 8167/9981 (82%)\n",
      "\n",
      "Train Epoch: 2 [0/84843 (0%)]\tLoss: 0.525475\n",
      "Train Epoch: 2 [10240/84843 (12%)]\tLoss: 0.548689\n",
      "Train Epoch: 2 [20480/84843 (24%)]\tLoss: 0.546032\n",
      "Train Epoch: 2 [30720/84843 (36%)]\tLoss: 0.479189\n",
      "Train Epoch: 2 [40960/84843 (48%)]\tLoss: 0.545988\n",
      "Train Epoch: 2 [51200/84843 (61%)]\tLoss: 0.575261\n",
      "Train Epoch: 2 [61440/84843 (73%)]\tLoss: 0.479898\n",
      "Train Epoch: 2 [71680/84843 (85%)]\tLoss: 0.497020\n",
      "Train Epoch: 2 [81920/84843 (97%)]\tLoss: 0.515329\n",
      "\n",
      "Training set: Average loss: 0.4989, Accuracy: 71450/84843 (84%)\n",
      "\n",
      "\n",
      "Validation Epoch: 2\tAccuracy: 8543/9981 (86%)\n",
      "\n",
      "Train Epoch: 3 [0/84843 (0%)]\tLoss: 0.445888\n",
      "Train Epoch: 3 [10240/84843 (12%)]\tLoss: 0.321839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [01:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mn_epoch) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m         train_accuracy_epoch\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m         valid_accuracy_epoch\u001b[38;5;241m.\u001b[39mappend(validation(model, epoch))\n\u001b[1;32m     15\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epoch, log_interval)\u001b[0m\n\u001b[1;32m      3\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     target \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sounds4th/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sounds4th/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sounds4th/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sounds4th/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mMFCC.__getitem__\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m     31\u001b[0m     fileid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_walker[n]\n\u001b[0;32m---> 32\u001b[0m     feature \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m fileid\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feature, label\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sounds4th/lib/python3.10/site-packages/torch/serialization.py:1005\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1003\u001b[0m orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n\u001b[1;32m   1004\u001b[0m overall_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1005\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[1;32m   1007\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1008\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m dispatching to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtorch.jit.load\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directly to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1009\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m silence this warning)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mUserWarning\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sounds4th/lib/python3.10/site-packages/torch/serialization.py:457\u001b[0m, in \u001b[0;36m_open_zipfile_reader.__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 457\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log_interval = 20\n",
    "n_epoch = 40\n",
    "\n",
    "# pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "\n",
    "train_accuracy_epoch = []\n",
    "valid_accuracy_epoch = []\n",
    "\n",
    "# The transform needs to live on the same device as the model and the data.\n",
    "# transform = transform.to(device)\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        train_accuracy_epoch.append(train(model, epoch, log_interval))\n",
    "        valid_accuracy_epoch.append(validation(model, epoch))\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save model\n",
    "        os.makedirs(\"./v2_models\", exist_ok=True)\n",
    "        torch.save(model.state_dict(), f\"./v2_models/model_e{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracies in the same plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_accuracy_epoch, label=\"Train\")\n",
    "plt.plot(valid_accuracy_epoch, label=\"Validation\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tensor):\n",
    "    model.eval()\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    # tensor = transform(tensor)\n",
    "    # if tensor shape is not 16000, then the waveform is too short and we need to pad it\n",
    "    if tensor.numel() != 16000:\n",
    "        tensor = F.pad(tensor, (0, 16000 - tensor.numel()), \"constant\", 0.0)\n",
    "\n",
    "    # # Calculate MFCC\n",
    "    tensor = extract_mfcc(tensor)\n",
    "    tensor = torch.squeeze(tensor, (0, 1)).t()\n",
    "\n",
    "    tensor = model(tensor.unsqueeze(0))\n",
    "    tensor = get_likely_index(tensor)\n",
    "    tensor = index_to_label(tensor.squeeze())\n",
    "    return tensor\n",
    "\n",
    "correct = 0\n",
    "test_pred = []\n",
    "test_target = []\n",
    "for i, (waveform, sample_rate, label, *_) in enumerate(test_set):\n",
    "    output = predict(waveform)\n",
    "    test_pred.append(output)\n",
    "    test_target.append(label)\n",
    "    if output == label:\n",
    "        correct += 1\n",
    "    # if output != label:\n",
    "    #     ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "    #     print(f\"Data point #{i}. Expected: {label}. Predicted: {output}.\")\n",
    "    #     # break\n",
    "# else:\n",
    "#     print(\"All examples in this dataset were correctly classified!\")\n",
    "#     print(\"In this case, let's just look at the last data point\")\n",
    "#     ipd.Audio(waveform.numpy(), rate=sample_rate)\n",
    "#     print(f\"Data point #{i}. Expected: {utterance}. Predicted: {output}.\")\n",
    "        \n",
    "print(f\"Accuracy: {correct}/{len(test_set)} ({100. * correct / len(test_set):.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix using pytorch\n",
    "from ignite.metrics.confusion_matrix import ConfusionMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "print(sd.query_devices())\n",
    "sd.default.device = \"Leo's iPhone 13 Microphone\"\n",
    "def record(seconds=5, sample_rate=16000):\n",
    "    # Make a 1s recording\n",
    "    print(\"Start recording.\")\n",
    "    recording = sd.rec(int(seconds * sample_rate), samplerate=sample_rate, channels=1)\n",
    "    sd.wait()\n",
    "    \n",
    "    # Define the file format\n",
    "    fileformat = \"wav\"\n",
    "    filename = f\"_audio.{fileformat}\"\n",
    "    # Write the recording to a file using scipy wavfile\n",
    "    wavfile.write(filename, sample_rate, recording)\n",
    "    return torchaudio.load(filename)\n",
    "\n",
    "# Detect whether notebook runs in google colab\n",
    "record_wav, sample_rate = record()\n",
    "# sample_rate, record_wav = wavfile.read(\"_audio.wav\")\n",
    "# Check if record_wav is a torch tensor\n",
    "if not isinstance(record_wav, torch.Tensor):\n",
    "    record_wav = torch.tensor(record_wav, dtype=torch.float32)\n",
    "record_wav = torch.reshape(record_wav, (1, -1))\n",
    "print(f\"Predicted: {predict(record_wav)}.\")\n",
    "ipd.Audio(record_wav, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(record_wav, torch.Tensor):\n",
    "    record_wav = torch.tensor(record_wav, dtype=torch.float32)\n",
    "record_wav = torch.reshape(record_wav, (1, -1))\n",
    "print(f\"Predicted: {predict(record_wav)}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sounds4th",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
